---
title: "PBSR Project"
author: "Aniket Saha"
date: "16/11/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=F,message=F,comment=NA,echo = FALSE)
```

## Question 1

$X$ denote the number of goals scored by home team in the premier league.
\newline Here $X$ is a random variable.
\newline Now, our task is to fit an appropriate probability model to $X$.
\newline Now, $X$ takes values in $\mathbb{N}_0=\{0,1,2,...\}$.
\newline 

### Geometric Distribution
First we will try to fit a geometric distribution to $X$.
\newline The general structure of a Geometric distribution is given by:
$$
\mathcal{S}=\{a, ar, ar^2, ar^3,...\}
$$
If we fit this distribution to $X$, then we will have:
\begin{itemize}
\item $P[X=0]=a$
\item $P[X=1]=ar$
\item $P[X=2]=ar^2$
\end{itemize}
And so on...
\newline Note that since $0\le P[X=k]\le1,\forall k\in \mathbb{N}_0$, we have:
$$
a>0,\hspace{0.2cm}r>0\hspace{0.3cm}[\because a,r\ne0]
$$
Now, since $\sum_{k=0}^{\infty}P[X=k]=1$, we have:
$$
\sum_{k=0}^{\infty}P[X=k]=P[X=0]+P[X=1]+P[X=2]+...
$$
$$
=a + ar+ ar^2+ ...
$$
$$
=a*(\frac{1}{1-r})
$$

Note that $a*\frac{1}{1-r}=1$.
Hence we have, $a=1-r \implies a+r=1$.

\newpage

### Conditions for $\mathcal{S}$ to be a Probability Distribution
Now, we get the required conditions for $\mathcal{S}$ to be the probability distribution of $X$. These are:
\begin{enumerate}
\item $a,r>0$
\item $a+r=1$
\end{enumerate}


### Finding $E[X]$
Now we need to check whether the mean and variance of $X$ exist.
\newline First of all we will try to find $E[X]$
$$E[X]=\sum_{k=0}^{\infty}k.P[X=k]$$
$$=\sum_{k=0}^{\infty}k.ar^k\hspace{0.3cm}[a+r=1]$$
$$=ar\sum_{k=0}^{\infty}k.r^{k-1}$$
We note that since $\sum_{k=0}^{\infty}r^k$ is summable as $0<r<1$, and the terms in $E[X]$ resembles $\frac{dr^k}{dr}=k.r^{k-1}$. We use the following method to find $E[X]$.
$$\sum_{k=0}^{\infty}r^k=\frac{1}{1-r}$$
Differentiating both sides with respect to r, we get:
$$\sum_{k=0}^{\infty}k.r^{k-1}=\frac{1}{(1-r)^2}=\frac{1}{a^2}$$
Multiplying $ar$ on both sides, we get:
$$ar\sum_{k=0}^{\infty}k.r^{k-1}=\frac{ar}{a^2}=\frac{r}{a}=\frac{r}{1-r}$$
This is the required $E[X]$.
\newpage

### Finding $V[X]$
Now $V[X]=E[X^2]-[E[X]]^2$
We will follow the same technique we used to find $E[X]$ to find $E[X^2]$
$$E[X^2]=\sum_{k=0}^{\infty}k^2.ar^k$$
We have,
$$\sum_{k=0}^{\infty}k.r^{k-1}=\frac{1}{(1-r)^2}=\frac{1}{a^2}\hspace{0.3cm}[From\ E[X]\ derivation]$$
Multiplying both sides with r, we get:
$$\sum_{k=0}^{\infty}k.r^{k}=\frac{r}{(1-r)^2}$$
Differentiating both sides with respect to $r$, we get:
$$\sum_{k=0}^{\infty}k^2.r^{k-1}=\frac{1+r}{(1-r)^3}$$
Now we multiply both sides with $ar$ to get $E[X^2]$:
$$E[X^2]=\sum_{k=0}^{\infty}k^2.ar^{k}=\frac{ar(1+r)}{(1-r)^3}$$
$$=\frac{r(1+r)}{(1-r)^2}\hspace{0.3cm}[\because a=1-r]$$
Hence $V[X]$ will be given by:
$$V[X]=E[X^2]-[E[X]]^2$$
$$=\frac{r(1+r)}{(1-r)^2}-[\frac{r}{1-r}]^2$$
$$=\frac{r}{(1-r)^2}$$
This is the required $V[X]$
\newpage

### Historical Data
We are given the following data:
\begin{itemize}
\item Mean=1.5
\item Median=1
\item Variance=2.25
\item Total number of matches=380
\end{itemize}
Now, we need to input this data in our probability model to get an estimate of $a$ and $r$.

We will use the method of moments to get $r$.
We have,
$$E[X]=\frac{r}{1-r}=mean=1.5=\frac{3}{2}$$
$$\implies 2r=3-3r$$
$$\implies 5r=3$$
$$\implies r=\frac{3}{5}$$
$$\therefore a=\frac{2}{5}$$

Now, we have: $X$ follows a geometric distribution with $a=2/5$ and $r=3/5$

### Finding Probabilities
To find $P[X\ge1]$
\\We have, $P[X\ge1]=1-P[X=0]\hspace{0.3cm}[\because total\ probability\ is\ 1]$
\newline $P[X\ge1]=1-a=1-\frac{2}{5}=\frac{3}{5}$
\newline This is the required probability that the home team will score at least 1 goal.
\newline To find $P[1\le X<4]$
\newline We have, $P[1\le X<4]=ar+ar^2+ar^3+ar^4$
\newline $$=\frac{2*3}{5*5}+\frac{2*3*3}{5*5*5}+\frac{2*3*3*3}{5*5*5*5}$$
\newline $$=\frac{6}{25}+\frac{18}{125}+\frac{54}{625}$$
\newline $$=`r (6/25)+(18/125)+(54/625)`$$
\newline This is the required probability that the home team will score at least 1 but less than 4 goals.

### Fitting a Poisson Model

Now we are to fit a Poisson model to $X$
Now the Poisson probability mass function with parameter $\lambda$ is:
$$f_X(x)=e^{-\lambda}\frac{\lambda^x}{x!}, x=0,1,2,...$$
Now, since the mean of a Poisson distribution is $\lambda$ itself, by the method of moments, we have:
$$E[X]=\lambda=1.5=mean$$

Therefore, X follows a Poisson(1.5) distribution here.
Now, we calculate the probabilities $P[X\ge1]$ and $P[1\le X<4]$.

$$P[X\ge1]=1-P[X=0]\hspace{0.3cm}[\because total\ probability\ is\ 1]$$
$$=1-e^{-1.5}.\frac{(1.5)^0}{0!}=1-e^{-1.5}$$
$$=`r 1-exp(-1.5)`$$
\newline This is the required probability that the home team will score at least 1 goal.
\newline To find $P[1\le X<4]$
$$P[1\le X<4]=\sum_{k=1}^{3}e^{-1.5}\frac{(1.5)^k}{k!}$$
$$=`r (1.5)*exp(-1.5)+((1.5*1.5)/2)*exp(-1.5)+((1.5*1.5*1.5)/6)*exp(-1.5)`$$

### Preference of Probability Model

First of all, both the models we have fitted here have only one independent parameter. However, the historical data had a unused data that could not be used since we used the method of moments to estimate the parameters. Now, the mean and variance of Poisson model is the same, 1.5 . The mean and variance of the geometric distribution are 1.5 and 3.75 respectively. Neither of which reflects the historical data satisfactorily due to the problem mentioned above.

However, to conclude, I would prefer the Poisson model than the geometric model as the $P[X\ge1]$ is much higher in case of Poisson model (=0.77) than that of geometric model  (=0.6).

Now, in the premier league, a few goals are usually scored in a match. And the home team gets on the scoresheet more often than not. So, it is expected that the home team will score in a particular game, and hence I have chosen the Poisson model as my preferred choice to describe $X$.

\newpage

## Question 2
We have, $X_1,X_2,...,X_n \sim Gamma(a,b)$, Here $X_i$'s are independent and identically distributed (iid).

Say, $X$ is a representative of $X_i$'s, then the probability density function of $X$ is given by:
$$f_X(x)=\frac{1}{b^a.\Gamma(a)}e^{-x/b}x^{a-1},\ \ x>0$$
Now, $E[X]=ab$ and $V[X]=ab^2$.

Also, to be noted that $a$ is the shape parameter and $b$ is the scale parameter.

### To find MLE of $\theta=log(a)$
We are to write an R code to get a function which gives us the value of MLE based on input data.

Now, likelihood function of the given sample is:
$$
L_X(a,b)=\prod_{i=1}^{n}[f_{X_i}(x)]\hspace{0.3cm}[\because the\ variables\ X_i's\ are\ independent]
$$
$$L_X(a,b)=\frac{1}{b^{an}}.e^{-\sum_ix_i/b}\prod_{i=1}^nx_i^{(a-1)}$$
Now we write the code to get MLE using optim function in R.
```{r}
da=rgamma(20,1.5,2.2)
m=c(1.5,2.2)
nll=function(x,d){
  l=0
  a=x[1]
  b=x[2]
  n=length(d)
  for(i in 1:n){
    l=l+log(dgamma(d[i],a,b))
  }
  return(-l)
}
MyMLE=function(c){
  return(log(optim(c,nll,d=da)$par[1]))
}
```

We have created the function MyMLE to compute the MLE of $\theta=log(a)$.
We have used the invariance property of MLE to get the MLE of $\theta=log(a)$.
\newpage

### Simulating Samples and Visualizing Sampling Distribution
Now we will simulate $X_1,X_2,...,X_n$ from $Gamma(1.5,2.2)$ distribution with n=20.

Then we will apply MyMLE to estimate $\theta$ in m=1000 cases and store the values in a vector.

Then we will draw a histogram to understand the nature of $\theta$.

```{r}
vec=c()
nll=function(x,d){
  l=0
  a=x[1]
  b=x[2]
  n=length(d)
  for(i in 1:n){
    l=l+log(dgamma(d[i],a,b))
  }
  return(-l)
}
MyMLE=function(c){
  return(log(optim(c,nll,d=da)$par[1]))
}
for(i in 1:1000){
  da=rgamma(20,1.5,2.2)
  vec=append(vec,MyMLE(c(1,2)))
}
hist(vec,xlab="Estimated value of log(a)",main="Histogram Showing Distribution of log(a)",xlim=c(-1,2))
abline(v=log(1.5),col="red")
```

Here the red vertical line indicates the true value of $log(a)=log(1.5)=$ `r log(1.5)`

Now the 2.5 and 97.5 percentile points from the distribution of $log(a)$ are 

```{r}
quantile(vec,probs=c(0.025,0.975))
```

The gap between the two percentile points is `r quantile(vec,probs=c(0.025,0.975))[2]-quantile(vec,probs=c(0.025,0.975))[1]`

\newpage

### Simulation Part 2

Now we change n to 40 and repeat the analysis that we did in the last part.

So we now have, $X_1,X_2,...,X_n$ from $Gamma(1.5,2.2)$ distribution with n=40.

Then we will apply MyMLE to estimate $\theta$ in m=1000 cases and store the values in a vector.

Then we will draw a histogram to understand the nature of $\theta$.

```{r}
vec=c()
nll=function(x,d){
  l=0
  a=x[1]
  b=x[2]
  n=length(d)
  for(i in 1:n){
    l=l+log(dgamma(d[i],a,b))
  }
  return(-l)
}
MyMLE=function(c){
  return(log(optim(c,nll,d=da)$par[1]))
}
for(i in 1:1000){
  da=rgamma(40,1.5,2.2)
  vec=append(vec,MyMLE(c(1,2)))
}
hist(vec,xlab="Estimated value of log(a)",main="Histogram Showing Distribution of log(a)",xlim=c(-1,2))
abline(v=log(1.5),col="red")
```


Here the red vertical line indicates the true value of $log(a)=log(1.5)=$ `r log(1.5)`.

Now the 2.5 and 97.5 percentile points from the distribution of $log(a)$ are 

```{r}
quantile(vec,probs=c(0.025,0.975))
```

The gap between the two percentile points is `r quantile(vec,probs=c(0.025,0.975))[2]-quantile(vec,probs=c(0.025,0.975))[1]`

\newpage

### Simulation Part 3

Now we change n to 100 and repeat the analysis that we did in the last part.

So we now have, $X_1,X_2,...,X_n$ from $Gamma(1.5,2.2)$ distribution with n=100.

Then we will apply MyMLE to estimate $\theta$ in m=1000 cases and store the values in a vector.

Then we will draw a histogram to understand the nature of $\theta$.

```{r}
vec=c()
nll=function(x,d){
  l=0
  a=x[1]
  b=x[2]
  n=length(d)
  for(i in 1:n){
    l=l+log(dgamma(d[i],a,b))
  }
  return(-l)
}
MyMLE=function(c){
  return(log(optim(c,nll,d=da)$par[1]))
}
for(i in 1:1000){
  da=rgamma(100,1.5,2.2)
  vec=append(vec,MyMLE(c(1,2)))
}
hist(vec,xlab="Estimated value of log(a)",main="Histogram Showing Distribution of log(a)",xlim=c(-1,2))
abline(v=log(1.5),col="red")
```


Here the red vertical line indicates the true value of $log(a)=log(1.5)=$ `r log(1.5)`.

Now the 2.5 and 97.5 percentile points from the distribution of $log(a)$ are 

```{r}
quantile(vec,probs=c(0.025,0.975))
```

The gap between the two percentile points is `r quantile(vec,probs=c(0.025,0.975))[2]-quantile(vec,probs=c(0.025,0.975))[1]`

\newpage

### Observation About Percentile Points

We observe that the percentile points of the distributions of $log(a)$ are as follows:

```{r}
library(lemon)
library(knitr)
df1=data.frame(Sample_Size=c(20,40,100),Percentile_Point_2.5=c(-0.05805598,0.06925598,0.167165),Percentile_point_97.5=c(1.17938445,0.90481612,0.705074), Difference=c(1.2374404,0.8355601,0.537909))
kable(df1,caption="Table for Showing Percentile Points and the Difference",align='c')
```

So we observe that the difference between the two percentile points are decreasing with the increase in sample size. 

\textbf{Comment:} This can be explained by the fact that the standard error of a statistic is inversely proportional with increasing sample size. And here $log(a)$ is a statistic (which is computed using MyMLE function). So as sample size increases, the variance of the distribution of $log(a)$ decreases.